{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import re, os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "\n",
    "import modlee\n",
    "from modlee import data_metafeatures as dmf\n",
    "from modlee.utils import text_loaders, image_loaders\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "DATA_ROOT = os.path.expanduser(\"~/efs/.data\")\n",
    "IMAGE_DATALOADER = modlee.utils.get_imagenette_dataloader()\n",
    "# TEXT_DATALOADER = modlee.utils.get_wnli_dataloader() \n",
    "\n",
    "\n",
    "TEXT_LOADERS = {loader_fn:getattr(text_loaders, loader_fn) for loader_fn in dir(text_loaders) if re.match('get_(.*)_dataloader', loader_fn)}\n",
    "IMAGE_LOADERS = [getattr(image_loaders, loader_fn) for loader_fn in dir(image_loaders) if re.match('get_(.*)_dataloader', loader_fn)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_cola_dataloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.conda/envs/modlee3.10/lib/python3.10/site-packages/modlee/data_metafeatures.py:778: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  embds = torch.Tensor(embds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_mnli_dataloader\n",
      "get_qnli_dataloader\n",
      "get_rte_dataloader\n",
      "get_sst2_dataloader\n",
      "get_stsb_dataloader\n",
      "get_wnli_dataloader\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = None\n",
    "features = []\n",
    "labels = []\n",
    "for text_loader_name,text_loader_fn in TEXT_LOADERS.items():\n",
    "    print(text_loader_name)\n",
    "    # for _ in range(10):\n",
    "    for _ in range(1):\n",
    "        text_mf = dmf.TextDataMetafeatures(text_loader_fn(), testing=True)\n",
    "        features.append({\n",
    "            **text_mf.embedding,\n",
    "            **text_mf.mfe,\n",
    "            **text_mf.properties\n",
    "        })\n",
    "        labels.append(text_loader_name)\n",
    "        # features.append(text_mf.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CoLA'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(TEXT_LOADERS.values())[0].args[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'get_mnli_dataloader'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(list(TEXT_LOADERS.values())[1])\n",
    "list(TEXT_LOADERS.values())[1].__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_global = None\n",
    "def get_df_from_loaders(loaders, modality, n_samples=1):\n",
    "    global mf_global\n",
    "    if isinstance(loaders, dict):\n",
    "        loaders = list(loaders.values())\n",
    "    df = pd.DataFrame()\n",
    "    print(loaders)\n",
    "    features = []\n",
    "    MFClass = getattr(dmf, f\"{modality.capitalize()}DataMetafeatures\")\n",
    "    for loader_fn in loaders:\n",
    "        for _ in range(n_samples):\n",
    "            metafeatures = MFClass(\n",
    "                loader_fn(root=DATA_ROOT), testing=True\n",
    "            )\n",
    "            if hasattr(loader_fn, 'args'):\n",
    "                dataset_name = loader_fn.args[0]\n",
    "            else:\n",
    "                dataset_name = loader_fn.__name__\n",
    "            mf_global = metafeatures\n",
    "            features.append({\n",
    "                    'dataset_name':dataset_name,\n",
    "                    **metafeatures.embedding,\n",
    "                    **metafeatures.mfe,\n",
    "                    **metafeatures.properties,\n",
    "            })\n",
    "            pd.DataFrame(features).to_csv(\n",
    "                f'./{modality}_features_cache.csv')\n",
    "    df = pd.DataFrame(features)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[functools.partial(<staticmethod(<function text_loaders._get_text_dataloader at 0x7f02c3f1f5b0>)>, 'CoLA', 527), <function text_loaders.get_mnli_dataloader at 0x7f02c3f1f640>, functools.partial(<staticmethod(<function text_loaders._get_text_dataloader at 0x7f02c3f1f5b0>)>, 'QNLI', 5463), functools.partial(<staticmethod(<function text_loaders._get_text_dataloader at 0x7f02c3f1f5b0>)>, 'RTE', 277), functools.partial(<staticmethod(<function text_loaders._get_text_dataloader at 0x7f02c3f1f5b0>)>, 'SST2', 872), functools.partial(<staticmethod(<function text_loaders._get_text_dataloader at 0x7f02c3f1f5b0>)>, 'STSB', 1500), functools.partial(<staticmethod(<function text_loaders._get_text_dataloader at 0x7f02c3f1f5b0>)>, 'WNLI', 71)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.conda/envs/modlee3.10/lib/python3.10/site-packages/modlee/data_metafeatures.py:778: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  embds = torch.Tensor(embds)\n"
     ]
    }
   ],
   "source": [
    "text_df = get_df_from_loaders(TEXT_LOADERS, 'text', n_samples=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cached_df = pd.read_csv('./image_features_cache_0.csv')\n",
    "\n",
    "class DFTransforms:\n",
    "    @staticmethod\n",
    "    def list_cols2item(df):\n",
    "        object_columns = df.select_dtypes(include=['object']).columns\n",
    "        df[object_columns] = df[object_columns].apply(\n",
    "            lambda x : x[0]\n",
    "        )\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def drop_nonnum(df):\n",
    "        return df.select_dtypes(include=['float','int'])\n",
    "        \n",
    "    @staticmethod\n",
    "    def fillna(df, val=0):\n",
    "        return df.fillna(val)\n",
    "    \n",
    "    @staticmethod\n",
    "    def dropna(df):\n",
    "        return df.dropna(axis=1, how='any')\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize(df):\n",
    "        def min_max_normalize(column):\n",
    "            return (column - column.min()) / (column.max() - column.min())\n",
    "        return df.apply(min_max_normalize)\n",
    "\n",
    "    @staticmethod\n",
    "    def compose(transforms):\n",
    "        def apply_transforms(df):\n",
    "            for transform in transforms:\n",
    "                df = transform(df)\n",
    "            return df\n",
    "        return apply_transforms\n",
    "df_transforms = DFTransforms.compose([\n",
    "    DFTransforms.list_cols2item,\n",
    "    DFTransforms.drop_nonnum,\n",
    "    DFTransforms.normalize,\n",
    "    DFTransforms.dropna,\n",
    "])\n",
    "def save_labels(df, fn):\n",
    "    with open(fn,'w') as _file:\n",
    "        _file.write('\\n'.join(list(df['dataset_name'])))\n",
    "\n",
    "def save_tsv(df, fn):\n",
    "    return df.to_csv(\n",
    "        fn,\n",
    "        sep='\\t',\n",
    "        index=False,\n",
    "        header=False \n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cached_df = pd.read_csv('./text_features_cache.csv')\n",
    "labels = list(cached_df['dataset_name'])\n",
    "save_labels(cached_df, './text_labels.txt')\n",
    "cached_df = df_transforms(cached_df)\n",
    "print(labels)\n",
    "save_tsv(cached_df, 'cached_text_metafeatures.tsv')\n",
    "# cached_df = DFTransforms.list_cols2item(cached_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[functools.partial(<staticmethod(<function image_loaders._get_image_dataloader at 0x7f168ef17490>)>, 'CIFAR10', train=False, download=True), functools.partial(<staticmethod(<function image_loaders._get_image_dataloader at 0x7f168ef17490>)>, 'DTD', split='test', download=True), functools.partial(<staticmethod(<function image_loaders._get_image_dataloader at 0x7f168ef17490>)>, 'EuroSAT', download=True), functools.partial(<staticmethod(<function image_loaders._get_image_dataloader at 0x7f168ef17490>)>, 'FashionMNIST', train=False, download=True), functools.partial(<staticmethod(<function image_loaders._get_image_dataloader at 0x7f168ef17490>)>, 'FGVCAircraft', split='test', download=True), functools.partial(<staticmethod(<function image_loaders._get_image_dataloader at 0x7f168ef17490>)>, 'Flowers102', split='test', download=True), functools.partial(<staticmethod(<function image_loaders._get_image_dataloader at 0x7f168ef17490>)>, 'GTSRB', split='test', download=True), functools.partial(<staticmethod(<function image_loaders._get_image_dataloader at 0x7f168ef17490>)>, 'Imagenette', split='val', size='full'), functools.partial(<staticmethod(<function image_loaders._get_image_dataloader at 0x7f168ef17490>)>, 'KMNIST', train=False, download=True), functools.partial(<staticmethod(<function image_loaders._get_image_dataloader at 0x7f168ef17490>)>, 'MNIST', train=False, download=True), functools.partial(<staticmethod(<function image_loaders._get_image_dataloader at 0x7f168ef17490>)>, 'Omniglot', download=True), functools.partial(<staticmethod(<function image_loaders._get_image_dataloader at 0x7f168ef17490>)>, 'OxfordIIITPet', split='test', download=True), functools.partial(<staticmethod(<function image_loaders._get_image_dataloader at 0x7f168ef17490>)>, 'Places365', split='val', small=True, download=True), functools.partial(<staticmethod(<function image_loaders._get_image_dataloader at 0x7f168ef17490>)>, 'QMNIST', what='test10k', download=True), functools.partial(<staticmethod(<function image_loaders._get_image_dataloader at 0x7f168ef17490>)>, 'RenderedSST2', split='test', download=True), functools.partial(<staticmethod(<function image_loaders._get_image_dataloader at 0x7f168ef17490>)>, 'SEMEION', download=True), functools.partial(<staticmethod(<function image_loaders._get_image_dataloader at 0x7f168ef17490>)>, 'STL10', split='test', download=True), functools.partial(<staticmethod(<function image_loaders._get_image_dataloader at 0x7f168ef17490>)>, 'SUN397', download=True), functools.partial(<staticmethod(<function image_loaders._get_image_dataloader at 0x7f168ef17490>)>, 'SVHN', split='test', download=True), functools.partial(<staticmethod(<function image_loaders._get_image_dataloader at 0x7f168ef17490>)>, 'USPS', train=False, download=True)]\n",
      "() {'train': False, 'download': True, 'transform': Compose(\n",
      "    Resize(size=(300, 300), interpolation=bilinear, max_size=None, antialias=True)\n",
      "    Grayscale(num_output_channels=3)\n",
      "    ToTensor()\n",
      ")}\n",
      "Files already downloaded and verified\n",
      "() {'train': False, 'download': True, 'transform': Compose(\n",
      "    Resize(size=(300, 300), interpolation=bilinear, max_size=None, antialias=True)\n",
      "    Grayscale(num_output_channels=3)\n",
      "    ToTensor()\n",
      ")}\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "image_df = get_df_from_loaders(IMAGE_LOADERS, 'image', n_samples=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>embd_0_mean_0</th>\n",
       "      <th>embd_0_mean_1</th>\n",
       "      <th>embd_0_mean_2</th>\n",
       "      <th>embd_0_mean_3</th>\n",
       "      <th>embd_0_mean_4</th>\n",
       "      <th>embd_0_mean_5</th>\n",
       "      <th>embd_0_mean_6</th>\n",
       "      <th>embd_0_mean_7</th>\n",
       "      <th>embd_0_mean_8</th>\n",
       "      <th>...</th>\n",
       "      <th>sparsity.sd_1</th>\n",
       "      <th>t_mean.mean_1</th>\n",
       "      <th>t_mean.sd_1</th>\n",
       "      <th>var.mean_1</th>\n",
       "      <th>var.sd_1</th>\n",
       "      <th>dataset_size</th>\n",
       "      <th>elem_0_shape</th>\n",
       "      <th>elem_0_dims</th>\n",
       "      <th>elem_1_shape</th>\n",
       "      <th>elem_1_dims</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CIFAR10</td>\n",
       "      <td>-0.016777</td>\n",
       "      <td>0.681486</td>\n",
       "      <td>-1.257155</td>\n",
       "      <td>-0.762495</td>\n",
       "      <td>-1.302225</td>\n",
       "      <td>1.646449</td>\n",
       "      <td>-1.796577</td>\n",
       "      <td>-2.000172</td>\n",
       "      <td>-1.442675</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.783333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.616162</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10000</td>\n",
       "      <td>[100, 3, 300, 300]</td>\n",
       "      <td>4</td>\n",
       "      <td>[100]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DTD</td>\n",
       "      <td>-1.058883</td>\n",
       "      <td>0.862001</td>\n",
       "      <td>0.060573</td>\n",
       "      <td>0.420286</td>\n",
       "      <td>0.459545</td>\n",
       "      <td>0.266008</td>\n",
       "      <td>0.723380</td>\n",
       "      <td>-0.358250</td>\n",
       "      <td>-0.699077</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.400000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>170.142525</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1888</td>\n",
       "      <td>[100, 3, 300, 300]</td>\n",
       "      <td>4</td>\n",
       "      <td>[100]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 310 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset_name  embd_0_mean_0  embd_0_mean_1  embd_0_mean_2  embd_0_mean_3  \\\n",
       "0      CIFAR10      -0.016777       0.681486      -1.257155      -0.762495   \n",
       "1          DTD      -1.058883       0.862001       0.060573       0.420286   \n",
       "\n",
       "   embd_0_mean_4  embd_0_mean_5  embd_0_mean_6  embd_0_mean_7  embd_0_mean_8  \\\n",
       "0      -1.302225       1.646449      -1.796577      -2.000172      -1.442675   \n",
       "1       0.459545       0.266008       0.723380      -0.358250      -0.699077   \n",
       "\n",
       "   ...  sparsity.sd_1  t_mean.mean_1  t_mean.sd_1  var.mean_1  var.sd_1  \\\n",
       "0  ...            NaN       4.783333          NaN    8.616162       NaN   \n",
       "1  ...            NaN      21.400000          NaN  170.142525       NaN   \n",
       "\n",
       "   dataset_size        elem_0_shape  elem_0_dims  elem_1_shape  elem_1_dims  \n",
       "0         10000  [100, 3, 300, 300]            4         [100]            1  \n",
       "1          1888  [100, 3, 300, 300]            4         [100]            1  \n",
       "\n",
       "[2 rows x 310 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m mf_dict \u001b[39m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[39m# **mf_global.embedding,\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[39m# **mf_global.mfe,\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmf_global\u001b[39m.\u001b[39mproperties\n\u001b[1;32m      5\u001b[0m }\n\u001b[0;32m----> 6\u001b[0m pd\u001b[39m.\u001b[39;49mDataFrame(mf_dict,)\n\u001b[1;32m      7\u001b[0m \u001b[39mprint\u001b[39m(mf_dict)\n",
      "File \u001b[0;32m~/.conda/envs/modlee3.10/lib/python3.10/site-packages/pandas/core/frame.py:767\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    761\u001b[0m     mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_mgr(\n\u001b[1;32m    762\u001b[0m         data, axes\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: columns}, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy\n\u001b[1;32m    763\u001b[0m     )\n\u001b[1;32m    765\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[1;32m    766\u001b[0m     \u001b[39m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 767\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, typ\u001b[39m=\u001b[39;49mmanager)\n\u001b[1;32m    768\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ma\u001b[39m.\u001b[39mMaskedArray):\n\u001b[1;32m    769\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mma\u001b[39;00m \u001b[39mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m~/.conda/envs/modlee3.10/lib/python3.10/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[39m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[39mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[39m=\u001b[39;49mdtype, typ\u001b[39m=\u001b[39;49mtyp, consolidate\u001b[39m=\u001b[39;49mcopy)\n",
      "File \u001b[0;32m~/.conda/envs/modlee3.10/lib/python3.10/site-packages/pandas/core/internals/construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    112\u001b[0m     \u001b[39m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         index \u001b[39m=\u001b[39m _extract_index(arrays)\n\u001b[1;32m    115\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         index \u001b[39m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/.conda/envs/modlee3.10/lib/python3.10/site-packages/pandas/core/internals/construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    675\u001b[0m lengths \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(raw_lengths))\n\u001b[1;32m    676\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(lengths) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 677\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAll arrays must be of the same length\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    679\u001b[0m \u001b[39mif\u001b[39;00m have_dicts:\n\u001b[1;32m    680\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    681\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    682\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "mf_dict = {\n",
    "    # **mf_global.embedding,\n",
    "    # **mf_global.mfe,\n",
    "    **mf_global.properties\n",
    "}\n",
    "pd.DataFrame(mf_dict,)\n",
    "print(mf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5.664189\n",
       "1    0.090764\n",
       "2    0.198000\n",
       "3   -0.078866\n",
       "4    9.513864\n",
       "5    1.288083\n",
       "6    0.530463\n",
       "Name: skewness.mean_0, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df['skewness.mean_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(features)\n",
    "# print(len(TEXT_LOADERS))\n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df.dtypes)\n",
    "import numpy as np\n",
    "object_columns = df.select_dtypes(include=['object']).columns\n",
    "df[object_columns] = df[object_columns].apply(\n",
    "    lambda x : x[0]\n",
    ")\n",
    "df.to_csv('text_metafeatures.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalize(column):\n",
    "    return (column - column.min()) / (column.max() - column.min())\n",
    "\n",
    "# Normalize DataFrame by columns\n",
    "normalized_df = df.apply(min_max_normalize)\n",
    "normalized_df.to_csv(\n",
    "    'text_metafeatures_normalized.tsv', \n",
    "    sep='\\t', \n",
    "    index=False,\n",
    "    header=False\n",
    "    )\n",
    "with open(\"data_labels.txt\",'w') as _file:\n",
    "    _file.write('\\n'.join(labels))\n",
    "    # _file.write('\\n'.join(list(TEXT_LOADERS.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['embd_mean_0', 'embd_mean_1', 'embd_mean_10', 'embd_mean_11', 'embd_mean_12', 'embd_mean_13', 'embd_mean_14', 'embd_mean_15', 'embd_mean_16', 'embd_mean_17', 'embd_mean_18', 'embd_mean_19', 'embd_mean_2', 'embd_mean_20', 'embd_mean_21', 'embd_mean_22', 'embd_mean_23', 'embd_mean_24', 'embd_mean_25', 'embd_mean_26', 'embd_mean_27', 'embd_mean_28', 'embd_mean_29', 'embd_mean_3', 'embd_mean_30', 'embd_mean_31', 'embd_mean_32', 'embd_mean_33', 'embd_mean_34', 'embd_mean_35', 'embd_mean_36', 'embd_mean_37', 'embd_mean_38', 'embd_mean_39', 'embd_mean_4', 'embd_mean_40', 'embd_mean_41', 'embd_mean_42', 'embd_mean_43', 'embd_mean_44', 'embd_mean_45', 'embd_mean_46', 'embd_mean_47', 'embd_mean_48', 'embd_mean_49', 'embd_mean_5', 'embd_mean_50', 'embd_mean_51', 'embd_mean_52', 'embd_mean_53', 'embd_mean_54', 'embd_mean_55', 'embd_mean_56', 'embd_mean_57', 'embd_mean_58', 'embd_mean_59', 'embd_mean_6', 'embd_mean_60', 'embd_mean_61', 'embd_mean_62', 'embd_mean_63', 'embd_mean_64', 'embd_mean_65', 'embd_mean_66', 'embd_mean_67', 'embd_mean_68', 'embd_mean_69', 'embd_mean_7', 'embd_mean_70', 'embd_mean_71', 'embd_mean_72', 'embd_mean_73', 'embd_mean_74', 'embd_mean_75', 'embd_mean_76', 'embd_mean_77', 'embd_mean_78', 'embd_mean_79', 'embd_mean_8', 'embd_mean_80', 'embd_mean_81', 'embd_mean_82', 'embd_mean_83', 'embd_mean_84', 'embd_mean_85', 'embd_mean_86', 'embd_mean_87', 'embd_mean_88', 'embd_mean_89', 'embd_mean_9', 'embd_mean_90', 'embd_mean_91', 'embd_mean_92', 'embd_mean_93', 'embd_mean_94', 'embd_mean_95', 'embd_std_0', 'embd_std_1', 'embd_std_10', 'embd_std_11', 'embd_std_12', 'embd_std_13', 'embd_std_14', 'embd_std_15', 'embd_std_16', 'embd_std_17', 'embd_std_18', 'embd_std_19', 'embd_std_2', 'embd_std_20', 'embd_std_21', 'embd_std_22', 'embd_std_23', 'embd_std_24', 'embd_std_25', 'embd_std_26', 'embd_std_27', 'embd_std_28', 'embd_std_29', 'embd_std_3', 'embd_std_30', 'embd_std_31', 'embd_std_32', 'embd_std_33', 'embd_std_34', 'embd_std_35', 'embd_std_36', 'embd_std_37', 'embd_std_38', 'embd_std_39', 'embd_std_4', 'embd_std_40', 'embd_std_41', 'embd_std_42', 'embd_std_43', 'embd_std_44', 'embd_std_45', 'embd_std_46', 'embd_std_47', 'embd_std_48', 'embd_std_49', 'embd_std_5', 'embd_std_50', 'embd_std_51', 'embd_std_52', 'embd_std_53', 'embd_std_54', 'embd_std_55', 'embd_std_56', 'embd_std_57', 'embd_std_58', 'embd_std_59', 'embd_std_6', 'embd_std_60', 'embd_std_61', 'embd_std_62', 'embd_std_63', 'embd_std_64', 'embd_std_65', 'embd_std_66', 'embd_std_67', 'embd_std_68', 'embd_std_69', 'embd_std_7', 'embd_std_70', 'embd_std_71', 'embd_std_72', 'embd_std_73', 'embd_std_74', 'embd_std_75', 'embd_std_76', 'embd_std_77', 'embd_std_78', 'embd_std_79', 'embd_std_8', 'embd_std_80', 'embd_std_81', 'embd_std_82', 'embd_std_83', 'embd_std_84', 'embd_std_85', 'embd_std_86', 'embd_std_87', 'embd_std_88', 'embd_std_89', 'embd_std_9', 'embd_std_90', 'embd_std_91', 'embd_std_92', 'embd_std_93', 'embd_std_94', 'embd_std_95']\n"
     ]
    }
   ],
   "source": [
    "embd_cols = sorted(col for col in normalized_df.columns if 'embd' in col)\n",
    "print(embd_cols)\n",
    "normalized_df[embd_cols].to_csv(\n",
    "    'text_metafeatures_normalized_embd.tsv',\n",
    "    sep='\\t',\n",
    "    index=False,\n",
    "    header=False\n",
    ")\n",
    "normalized_df.drop(columns=embd_cols).to_csv(\n",
    "    'text_metafeatures_normalized_mfe.tsv',\n",
    "    sep='\\t',\n",
    "    index=False,\n",
    "    header=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "!code ./text_metafeatures.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['get_cola_dataloader', 'get_mnli_dataloader', 'get_qnli_dataloader', 'get_rte_dataloader', 'get_sst2_dataloader', 'get_stsb_dataloader', 'get_wnli_dataloader']\n"
     ]
    }
   ],
   "source": [
    "print(list(TEXT_LOADERS.keys()), sep='\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modlee3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
