{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/mansiagr4/gifs/raw/main/new_small_logo.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Text Embeddings With Tabular Classification Model\n",
    "\n",
    "This tutorial will guide you through a step-by-step breakdown of using a Multilayer Perceptron (MLP) with embeddings from a pre-trained `DistilBERT` model to classify text sentiment from the IMDB movie reviews dataset. We'll cover everything from dataset preprocessing to model evaluation, explaining each part in detail.\n",
    "\n",
    "[![Open in Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/code/modlee/modlee-text-embeddings)\n",
    "\n",
    "\n",
    "In this section, we import the necessary libraries from `PyTorch` and `Torchvision`.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import modlee\n",
    "import lightning.pytorch as pl\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will set our Modlee API key and initialize the Modlee package.\n",
    "Make sure that you have a Modlee account and an API key [from the dashboard](https://www.dashboard.modlee.ai/).\n",
    "Replace `replace-with-your-api-key` with your API key.\n",
    "```python\n",
    "import os\n",
    "\n",
    "os.environ['MODLEE_API_KEY'] = \"replace-with-your-api-key\"\n",
    "modlee.init(api_key=os.environ['MODLEE_API_KEY'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLP (Multilayer Perceptron) model is defined here as a neural network with three fully connected linear layers. Each layer is followed by a `ReLU` activation function.\n",
    "\n",
    "```python\n",
    "class MLP(modlee.model.TabularClassificationModleeModel):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 256),  \n",
    "            nn.ReLU(),                   \n",
    "            nn.Linear(256, 128),        \n",
    "            nn.ReLU(),                  \n",
    "            nn.Linear(128, num_classes)   \n",
    "        )\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y_target = batch\n",
    "        y_pred = self(x)\n",
    "        loss = self.loss_fn(y_pred, y_target)\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y_target = val_batch\n",
    "        y_pred = self(x)\n",
    "        val_loss = self.loss_fn(y_pred, y_target)  \n",
    "        return {'val_loss': val_loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=0.001, momentum=0.9)  \n",
    "        return optimizer\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we load `DistilBERT`, which is a more compact version of `BERT`. The tokenizer is responsible for converting raw text into a format that the `DistilBERT` model can understand.\n",
    "\n",
    "```python\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the pre-trained DistilBERT tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "bert = DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the IMDB dataset using the Hugging Face datasets library. \n",
    "\n",
    "```python\n",
    "dataset = load_dataset('imdb')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since processing the entire dataset can be slow, we sample a subset of 1000 examples from the dataset to speed up computation.\n",
    "\n",
    "```python\n",
    "def sample_subset(dataset, subset_size=1000):\n",
    "    # Randomly shuffle dataset indices and select a subset\n",
    "    sample_indices = torch.randperm(len(dataset))[:subset_size]\n",
    "    # Select the sampled data based on the shuffled indices\n",
    "    sampled_data = dataset.select(sample_indices.tolist())\n",
    "\n",
    "    return sampled_data\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DistilBERT` turns text into numerical embeddings that the model can understand. We first preprocess the text by tokenizing and padding it. Then, `DistilBERT` generates embeddings for each sentence.\n",
    "\n",
    "\n",
    "```python\n",
    "def get_text_embeddings(texts, tokenizer, bert, device, max_length=128):\n",
    "\n",
    "    # Tokenize the input texts, with padding and truncation to a fixed max length\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, \n",
    "                        truncation=True, max_length=max_length)\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "    # Get the embeddings from BERT without calculating gradients\n",
    "    with torch.no_grad():\n",
    "        embeddings = bert(input_ids, attention_mask=attention_mask).last_hidden_state.mean(dim=1)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# Precompute embeddings for the entire dataset\n",
    "def precompute_embeddings(dataset, tokenizer, bert, device, max_length=128):\n",
    "    texts = dataset['text'] \n",
    "    embeddings = get_text_embeddings(texts, tokenizer, bert, device, max_length)\n",
    "    return embeddings\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `train_test_split` to split the precomputed embeddings and their corresponding labels into training and validation sets.\n",
    "\n",
    "```python\n",
    "def split_data(embeddings, labels):\n",
    "    # Split the embeddings and labels into training and validation sets \n",
    "    train_embeddings, val_embeddings, train_labels, val_labels = train_test_split(\n",
    "        embeddings, labels, test_size=0.2, random_state=42)\n",
    "    \n",
    "    return train_embeddings, val_embeddings, train_labels, val_labels\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training and validation data are batched using the `PyTorch DataLoader`, which ensures efficient processing during training.\n",
    "\n",
    "```python\n",
    "def create_dataloaders(train_embeddings, train_labels, val_embeddings, val_labels, batch_size):\n",
    "    # Create TensorDataset objects for training and validation data\n",
    "    train_dataset = TensorDataset(train_embeddings, train_labels)\n",
    "    val_dataset = TensorDataset(val_embeddings, val_labels)\n",
    "\n",
    "    # Create DataLoader objects to handle batching and shuffling of data\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) \n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False) \n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train_model` function defines the training loop. After training, the model is evaluated on the validation set.\n",
    "\n",
    "```python\n",
    "def train_model(modlee_model, train_dataloader, val_dataloader, num_epochs=1):\n",
    "    with modlee.start_run() as run:\n",
    "        # Create a PyTorch Lightning trainer\n",
    "        trainer = pl.Trainer(max_epochs=num_epochs)\n",
    "\n",
    "        # Train the model using the training and validation data loaders\n",
    "        trainer.fit(\n",
    "            model=modlee_model,\n",
    "            train_dataloaders=train_dataloader,\n",
    "            val_dataloaders=val_dataloader\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we run the script, which follows these steps: loading and sampling the dataset, precomputing embeddings, training the MLP, and evaluating the model.\n",
    "\n",
    "```python\n",
    "# Load and preprocess a subset of the IMDB dataset\n",
    "train_data = sample_subset(dataset['train'], subset_size=1000) \n",
    "test_data = sample_subset(dataset['test'], subset_size=1000)  \n",
    "\n",
    "# Precompute BERT embeddings to speed up training\n",
    "print(\"Precomputing embeddings for training and testing data...\")\n",
    "train_embeddings = precompute_embeddings(train_data, tokenizer, bert, device) \n",
    "test_embeddings = precompute_embeddings(test_data, tokenizer, bert, device) \n",
    "\n",
    "# Convert labels from lists to tensors\n",
    "train_labels = torch.tensor(train_data['label'], dtype=torch.long) \n",
    "test_labels = torch.tensor(test_data['label'], dtype=torch.long) \n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "train_embeddings, val_embeddings, train_labels, val_labels = split_data(train_embeddings, train_labels) \n",
    "\n",
    "# Create DataLoader instances for batching data\n",
    "batch_size = 32  # Define batch size\n",
    "train_loader, val_loader = create_dataloaders(train_embeddings, train_labels, val_embeddings, val_labels, batch_size)  \n",
    "\n",
    "# Initialize and train the MLP model\n",
    "input_size = 768  \n",
    "num_classes = 2   \n",
    "mlp_text = MLP(input_size=input_size, num_classes=num_classes).to(device) \n",
    "\n",
    "print(\"Starting training...\")\n",
    "train_model(mlp_text, train_loader, val_loader, num_epochs=5) \n",
    "\n",
    "```\n",
    "\n",
    "We can view the saved training assests. With Modlee, your training assets are automatically saved, preserving valuable insights for future reference and collaboration.\n",
    "\n",
    "```python\n",
    "last_run_path = modlee.last_run_path()\n",
    "print(f\"Run path: {last_run_path}\")\n",
    "artifacts_path = os.path.join(last_run_path, 'artifacts')\n",
    "artifacts = sorted(os.listdir(artifacts_path))\n",
    "print(f\"Saved artifacts: {artifacts}\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
