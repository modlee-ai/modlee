{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZlD9MHpyLdn"
      },
      "source": [
        "# **Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZE0CHU9SkIC"
      },
      "source": [
        "In this exercise, you will implement the `modlee` package to:\n",
        "- Document an image segmentation experiment with a pretrained model from `torchvision`.\n",
        "- Receive and train a recommended model.\n",
        "\n",
        "It may be helpful to [keep the API documentation open](https://www.documentation.modlee.ai/index.html).\n",
        "\n",
        "For best performance, ensure that the runtime is set to use a GPU (`Runtime > Change runtime type > T4 GPU`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBfCjnQCtL_6"
      },
      "source": [
        "# **Installation**\n",
        "\n",
        "First, we need to install `modlee` and its related packages.\n",
        "Make sure that you have an account and API key [from the dashboard](https://www.dashboard.modlee.ai/).\n",
        "Replace `\"replace-with-your-api-key\"` with your API key.\n",
        "Run the following three cells; they will execute successively.\n",
        "This process may take a few minutes, so you can [review the examples](https://www.documentation.modlee.ai/notebooks/document.html) while waiting.\n",
        "\n",
        "*__NOTE:__ if you receive a message to \"restart the runtime in order to use newly installed versions,\" you can safely \"cancel.\"*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7ZfOmEtoZJuu",
        "outputId": "96b675d4-1ba4-4d55-f95c-ce934ee2d6f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1944  100  1944    0     0   5102      0 --:--:-- --:--:-- --:--:--  5115\n",
            "Requirement already satisfied: aiohttp==3.9.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (3.9.3)\n",
            "Requirement already satisfied: aiosignal==1.3.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (1.3.1)\n",
            "Collecting alembic==1.13.1 (from -r requirements.txt (line 3))\n",
            "  Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types==0.6.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (0.6.0)\n",
            "Collecting anyio==4.3.0 (from -r requirements.txt (line 5))\n",
            "  Downloading anyio-4.3.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.6/85.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting arrow==1.3.0 (from -r requirements.txt (line 6))\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: async-timeout==4.0.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (4.0.3)\n",
            "Requirement already satisfied: attrs==23.2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (23.2.0)\n",
            "Collecting backoff==2.2.1 (from -r requirements.txt (line 9))\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: beautifulsoup4==4.12.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (4.12.3)\n",
            "Collecting blessed==1.20.0 (from -r requirements.txt (line 11))\n",
            "  Downloading blessed-1.20.0-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting boto3==1.34.66 (from -r requirements.txt (line 12))\n",
            "  Downloading boto3-1.34.66-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore==1.34.66 (from -r requirements.txt (line 13))\n",
            "  Downloading botocore-1.34.66-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi==2024.2.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (2024.2.2)\n",
            "Requirement already satisfied: charset-normalizer==3.3.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 15)) (3.3.2)\n",
            "Requirement already satisfied: click==8.1.7 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 16)) (8.1.7)\n",
            "Requirement already satisfied: cloudpickle==2.2.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 17)) (2.2.1)\n",
            "Collecting cmake==3.28.3 (from -r requirements.txt (line 18))\n",
            "  Downloading cmake-3.28.3-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.3/26.3 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy==1.2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 19)) (1.2.0)\n",
            "Collecting croniter==1.4.1 (from -r requirements.txt (line 20))\n",
            "  Downloading croniter-1.4.1-py2.py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: cycler==0.12.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 21)) (0.12.1)\n",
            "Collecting databricks-cli==0.18.0 (from -r requirements.txt (line 22))\n",
            "  Downloading databricks_cli-0.18.0-py2.py3-none-any.whl (150 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.3/150.3 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting deepdiff==6.7.1 (from -r requirements.txt (line 23))\n",
            "  Downloading deepdiff-6.7.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.6/76.6 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker==6.1.3 (from -r requirements.txt (line 24))\n",
            "  Downloading docker-6.1.3-py3-none-any.whl (148 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.1/148.1 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting editor==1.6.6 (from -r requirements.txt (line 25))\n",
            "  Downloading editor-1.6.6-py3-none-any.whl (4.0 kB)\n",
            "Requirement already satisfied: entrypoints==0.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 26)) (0.4)\n",
            "Requirement already satisfied: exceptiongroup==1.2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 27)) (1.2.0)\n",
            "Collecting fastapi==0.110.0 (from -r requirements.txt (line 28))\n",
            "  Downloading fastapi-0.110.0-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting filelock==3.13.1 (from -r requirements.txt (line 29))\n",
            "  Downloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: Flask==2.2.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 30)) (2.2.5)\n",
            "Requirement already satisfied: fonttools==4.50.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 31)) (4.50.0)\n",
            "Requirement already satisfied: frozenlist==1.4.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 32)) (1.4.1)\n",
            "Collecting fsspec==2024.3.1 (from -r requirements.txt (line 33))\n",
            "  Downloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gitdb==4.0.11 (from -r requirements.txt (line 34))\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting GitPython==3.1.42 (from -r requirements.txt (line 35))\n",
            "  Downloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gower==0.1.2 (from -r requirements.txt (line 36))\n",
            "  Downloading gower-0.1.2-py3-none-any.whl (5.2 kB)\n",
            "Requirement already satisfied: greenlet==3.0.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 37)) (3.0.3)\n",
            "Collecting gunicorn==21.2.0 (from -r requirements.txt (line 38))\n",
            "  Downloading gunicorn-21.2.0-py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.2/80.2 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11==0.14.0 (from -r requirements.txt (line 39))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna==3.6 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 40)) (3.6)\n",
            "Collecting igraph==0.11.4 (from -r requirements.txt (line 41))\n",
            "  Downloading igraph-0.11.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting importlib-metadata==6.11.0 (from -r requirements.txt (line 42))\n",
            "  Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: iniconfig==2.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 43)) (2.0.0)\n",
            "Collecting inquirer==3.2.4 (from -r requirements.txt (line 44))\n",
            "  Downloading inquirer-3.2.4-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: itsdangerous==2.1.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 45)) (2.1.2)\n",
            "Requirement already satisfied: Jinja2==3.1.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 46)) (3.1.3)\n",
            "Collecting jmespath==1.0.1 (from -r requirements.txt (line 47))\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: joblib==1.3.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 48)) (1.3.2)\n",
            "Requirement already satisfied: kiwisolver==1.4.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 49)) (1.4.5)\n",
            "Collecting lit==18.1.1 (from -r requirements.txt (line 50))\n",
            "  Downloading lit-18.1.1.tar.gz (161 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.1/161.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting Mako==1.3.2 (from -r requirements.txt (line 51))\n",
            "  Downloading Mako-1.3.2-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Markdown==3.6 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 52)) (3.6)\n",
            "Requirement already satisfied: markdown-it-py==3.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 53)) (3.0.0)\n",
            "Requirement already satisfied: MarkupSafe==2.1.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 54)) (2.1.5)\n",
            "Requirement already satisfied: mdurl==0.1.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 55)) (0.1.2)\n",
            "Collecting mlflow==2.6.0 (from -r requirements.txt (line 56))\n",
            "  Downloading mlflow-2.6.0-py3-none-any.whl (18.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath==1.3.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 57)) (1.3.0)\n",
            "Requirement already satisfied: multidict==6.0.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 58)) (6.0.5)\n",
            "Requirement already satisfied: networkx==3.2.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 59)) (3.2.1)\n",
            "Requirement already satisfied: oauthlib==3.2.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 60)) (3.2.2)\n",
            "Collecting onnx==1.14.1 (from -r requirements.txt (line 61))\n",
            "  Downloading onnx-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m100.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ordered-set==4.1.0 (from -r requirements.txt (line 62))\n",
            "  Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
            "Collecting packaging==23.2 (from -r requirements.txt (line 63))\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas==2.2.1 (from -r requirements.txt (line 64))\n",
            "  Downloading pandas-2.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: patsy==0.5.6 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 65)) (0.5.6)\n",
            "Requirement already satisfied: pluggy==1.4.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 66)) (1.4.0)\n",
            "Collecting protobuf==4.25.3 (from -r requirements.txt (line 67))\n",
            "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting psutil==5.9.8 (from -r requirements.txt (line 68))\n",
            "  Downloading psutil-5.9.8-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyarrow==12.0.1 (from -r requirements.txt (line 69))\n",
            "  Downloading pyarrow-12.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.9/38.9 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic==1.10.12 (from -r requirements.txt (line 70))\n",
            "  Downloading pydantic-1.10.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m104.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic_core==2.4.0 (from -r requirements.txt (line 71))\n",
            "  Downloading pydantic_core-2.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Pygments==2.17.2 (from -r requirements.txt (line 72))\n",
            "  Downloading pygments-2.17.2-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyJWT==2.8.0 (from -r requirements.txt (line 73))\n",
            "  Downloading PyJWT-2.8.0-py3-none-any.whl (22 kB)\n",
            "Collecting pymfe==0.4.2 (from -r requirements.txt (line 74))\n",
            "  Downloading pymfe-0.4.2-py3-none-any.whl (155 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyparsing==3.1.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 75)) (3.1.2)\n",
            "Collecting pytest==8.1.1 (from -r requirements.txt (line 76))\n",
            "  Downloading pytest-8.1.1-py3-none-any.whl (337 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.4/337.4 kB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dateutil==2.9.0.post0 (from -r requirements.txt (line 77))\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-multipart==0.0.9 (from -r requirements.txt (line 78))\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: pytz==2023.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 79)) (2023.4)\n",
            "Requirement already satisfied: PyYAML==6.0.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 80)) (6.0.1)\n",
            "Collecting querystring-parser==1.2.4 (from -r requirements.txt (line 81))\n",
            "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
            "Collecting readchar==4.0.6 (from -r requirements.txt (line 82))\n",
            "  Downloading readchar-4.0.6-py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: requests==2.31.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 83)) (2.31.0)\n",
            "Requirement already satisfied: rich==13.7.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 84)) (13.7.1)\n",
            "Collecting runs==1.2.2 (from -r requirements.txt (line 85))\n",
            "  Downloading runs-1.2.2-py3-none-any.whl (7.0 kB)\n",
            "Collecting s3transfer==0.10.1 (from -r requirements.txt (line 86))\n",
            "  Downloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-learn==1.4.1.post1 (from -r requirements.txt (line 87))\n",
            "  Downloading scikit_learn-1.4.1.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m111.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six==1.16.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 88)) (1.16.0)\n",
            "Collecting smmap==5.0.1 (from -r requirements.txt (line 89))\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: sniffio==1.3.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 90)) (1.3.1)\n",
            "Requirement already satisfied: soupsieve==2.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 91)) (2.5)\n",
            "Collecting SQLAlchemy==2.0.28 (from -r requirements.txt (line 92))\n",
            "  Downloading SQLAlchemy-2.0.28-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sqlparse==0.4.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 93)) (0.4.4)\n",
            "Collecting starlette==0.36.3 (from -r requirements.txt (line 94))\n",
            "  Downloading starlette-0.36.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starsessions==1.3.0 (from -r requirements.txt (line 95))\n",
            "  Downloading starsessions-1.3.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: statsmodels==0.14.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 96)) (0.14.1)\n",
            "Requirement already satisfied: sympy==1.12 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 97)) (1.12)\n",
            "Requirement already satisfied: tabulate==0.9.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 98)) (0.9.0)\n",
            "Collecting texttable==1.7.0 (from -r requirements.txt (line 99))\n",
            "  Downloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: threadpoolctl==3.4.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 100)) (3.4.0)\n",
            "Requirement already satisfied: tomli==2.0.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 101)) (2.0.1)\n",
            "Requirement already satisfied: torchsummary==1.5.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 102)) (1.5.1)\n",
            "Requirement already satisfied: tqdm==4.66.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 103)) (4.66.2)\n",
            "Collecting traitlets==5.14.2 (from -r requirements.txt (line 104))\n",
            "  Downloading traitlets-5.14.2-py3-none-any.whl (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting types-python-dateutil==2.9.0.20240316 (from -r requirements.txt (line 105))\n",
            "  Downloading types_python_dateutil-2.9.0.20240316-py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: typing_extensions==4.10.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 106)) (4.10.0)\n",
            "Collecting tzdata==2024.1 (from -r requirements.txt (line 107))\n",
            "  Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3==2.2.1 (from -r requirements.txt (line 108))\n",
            "  Downloading urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn==0.29.0 (from -r requirements.txt (line 109))\n",
            "  Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wcwidth==0.2.13 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 110)) (0.2.13)\n",
            "Requirement already satisfied: websocket-client==1.7.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 111)) (1.7.0)\n",
            "Collecting websockets==12.0 (from -r requirements.txt (line 112))\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Werkzeug==3.0.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 113)) (3.0.1)\n",
            "Collecting xmod==1.8.1 (from -r requirements.txt (line 114))\n",
            "  Downloading xmod-1.8.1-py3-none-any.whl (4.6 kB)\n",
            "Requirement already satisfied: yarl==1.9.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 115)) (1.9.4)\n",
            "Requirement already satisfied: zipp==3.18.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 116)) (3.18.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.20 in /usr/local/lib/python3.10/dist-packages (from contourpy==1.2.0->-r requirements.txt (line 19)) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gower==0.1.2->-r requirements.txt (line 36)) (1.11.4)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.10/dist-packages (from mlflow==2.6.0->-r requirements.txt (line 56)) (3.7.1)\n",
            "Requirement already satisfied: setuptools>=41.0 in /usr/local/lib/python3.10/dist-packages (from readchar==4.0.6->-r requirements.txt (line 82)) (67.7.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow==2.6.0->-r requirements.txt (line 56)) (9.4.0)\n",
            "Building wheels for collected packages: lit\n",
            "  Building wheel for lit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lit: filename=lit-18.1.1-py3-none-any.whl size=96363 sha256=c5bfd705a9c269d837941f3a1a90116d9461c6051981f4a17cbf39d4f34b419b\n",
            "  Stored in directory: /root/.cache/pip/wheels/1d/74/6b/88e95944e9f9078f1dc1c0f634a542efb4d26ecae6000ca8cf\n",
            "Successfully built lit\n",
            "Installing collected packages: texttable, lit, cmake, xmod, websockets, urllib3, tzdata, types-python-dateutil, traitlets, SQLAlchemy, smmap, readchar, querystring-parser, python-multipart, python-dateutil, PyJWT, Pygments, pydantic_core, pydantic, pyarrow, psutil, protobuf, packaging, ordered-set, Mako, jmespath, importlib-metadata, igraph, h11, fsspec, filelock, blessed, backoff, anyio, uvicorn, starlette, scikit-learn, runs, pytest, pandas, onnx, gunicorn, gower, gitdb, deepdiff, croniter, botocore, arrow, alembic, starsessions, s3transfer, GitPython, fastapi, editor, docker, databricks-cli, pymfe, mlflow, inquirer, boto3\n",
            "  Attempting uninstall: cmake\n",
            "    Found existing installation: cmake 3.27.9\n",
            "    Uninstalling cmake-3.27.9:\n",
            "      Successfully uninstalled cmake-3.27.9\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "  Attempting uninstall: traitlets\n",
            "    Found existing installation: traitlets 5.7.1\n",
            "    Uninstalling traitlets-5.7.1:\n",
            "      Successfully uninstalled traitlets-5.7.1\n",
            "  Attempting uninstall: SQLAlchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.29\n",
            "    Uninstalling SQLAlchemy-2.0.29:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.29\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.8.2\n",
            "    Uninstalling python-dateutil-2.8.2:\n",
            "      Successfully uninstalled python-dateutil-2.8.2\n",
            "  Attempting uninstall: PyJWT\n",
            "    Found existing installation: PyJWT 2.3.0\n",
            "    Uninstalling PyJWT-2.3.0:\n",
            "      Successfully uninstalled PyJWT-2.3.0\n",
            "  Attempting uninstall: Pygments\n",
            "    Found existing installation: Pygments 2.16.1\n",
            "    Uninstalling Pygments-2.16.1:\n",
            "      Successfully uninstalled Pygments-2.16.1\n",
            "  Attempting uninstall: pydantic_core\n",
            "    Found existing installation: pydantic_core 2.16.3\n",
            "    Uninstalling pydantic_core-2.16.3:\n",
            "      Successfully uninstalled pydantic_core-2.16.3\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.6.4\n",
            "    Uninstalling pydantic-2.6.4:\n",
            "      Successfully uninstalled pydantic-2.6.4\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.9.5\n",
            "    Uninstalling psutil-5.9.5:\n",
            "      Successfully uninstalled psutil-5.9.5\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib_metadata 7.1.0\n",
            "    Uninstalling importlib_metadata-7.1.0:\n",
            "      Successfully uninstalled importlib_metadata-7.1.0\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2023.6.0\n",
            "    Uninstalling fsspec-2023.6.0:\n",
            "      Successfully uninstalled fsspec-2023.6.0\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.13.3\n",
            "    Uninstalling filelock-3.13.3:\n",
            "      Successfully uninstalled filelock-3.13.3\n",
            "  Attempting uninstall: anyio\n",
            "    Found existing installation: anyio 3.7.1\n",
            "    Uninstalling anyio-3.7.1:\n",
            "      Successfully uninstalled anyio-3.7.1\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "  Attempting uninstall: pytest\n",
            "    Found existing installation: pytest 7.4.4\n",
            "    Uninstalling pytest-7.4.4:\n",
            "      Successfully uninstalled pytest-7.4.4\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.5.3\n",
            "    Uninstalling pandas-1.5.3:\n",
            "      Successfully uninstalled pandas-1.5.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-nccl-cu12==2.19.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "gcsfs 2023.6.0 requires fsspec==2023.6.0, but you have fsspec 2024.3.1 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 2.2.1 which is incompatible.\n",
            "jupyter-server 1.24.0 requires anyio<4,>=3.1.0, but you have anyio 4.3.0 which is incompatible.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.25.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed GitPython-3.1.42 Mako-1.3.2 PyJWT-2.8.0 Pygments-2.17.2 SQLAlchemy-2.0.28 alembic-1.13.1 anyio-4.3.0 arrow-1.3.0 backoff-2.2.1 blessed-1.20.0 boto3-1.34.66 botocore-1.34.66 cmake-3.28.3 croniter-1.4.1 databricks-cli-0.18.0 deepdiff-6.7.1 docker-6.1.3 editor-1.6.6 fastapi-0.110.0 filelock-3.13.1 fsspec-2024.3.1 gitdb-4.0.11 gower-0.1.2 gunicorn-21.2.0 h11-0.14.0 igraph-0.11.4 importlib-metadata-6.11.0 inquirer-3.2.4 jmespath-1.0.1 lit-18.1.1 mlflow-2.6.0 onnx-1.14.1 ordered-set-4.1.0 packaging-23.2 pandas-2.2.1 protobuf-4.25.3 psutil-5.9.8 pyarrow-12.0.1 pydantic-1.10.12 pydantic_core-2.4.0 pymfe-0.4.2 pytest-8.1.1 python-dateutil-2.9.0.post0 python-multipart-0.0.9 querystring-parser-1.2.4 readchar-4.0.6 runs-1.2.2 s3transfer-0.10.1 scikit-learn-1.4.1.post1 smmap-5.0.1 starlette-0.36.3 starsessions-1.3.0 texttable-1.7.0 traitlets-5.14.2 types-python-dateutil-2.9.0.20240316 tzdata-2024.1 urllib3-2.2.1 uvicorn-0.29.0 websockets-12.0 xmod-1.8.1\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "baea44d70de54e46a04cb0428207b4b1",
              "pip_warning": {
                "packages": [
                  "dateutil",
                  "psutil"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import os\n",
        "os.environ['MODLEE_API_KEY'] = \"replace-with-your-api-key\"\n",
        "!curl -H X-API-KEY:$MODLEE_API_KEY https://server.modlee.ai:7070/get_wheel/requirements.txt -O\n",
        "!pip3 install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M69SyWAlr6M-"
      },
      "outputs": [],
      "source": [
        "!curl -H X-API-KEY:$MODLEE_API_KEY https://server.modlee.ai:7070/get_wheel/modlee-0.0.1.post6-py3-none-any.whl -O\n",
        "!curl -H X-API-KEY:$MODLEE_API_KEY https://server.modlee.ai:7070/get_wheel/onnx2torch-1.5.11-py3-none-any.whl -O\n",
        "!curl -H X-API-KEY:$MODLEE_API_KEY https://server.modlee.ai:7070/get_wheel/onnx_graphsurgeon-0.3.27-py2.py3-none-any.whl -O\n",
        "!pip3 install --force-reinstall --no-deps modlee-0.0.1.post6-py3-none-any.whl onnx2torch-1.5.11-py3-none-any.whl onnx_graphsurgeon-0.3.27-py2.py3-none-any.whl \\\n",
        "    lightning==2.0.7 pytorch-lightning==2.0.7 lightning-utilities lightning-cloud torchmetrics==1.3.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66FxwqH_SkID"
      },
      "outputs": [],
      "source": [
        "# Boilerplate imports\n",
        "import os, sys\n",
        "import ssl\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "import lightning.pytorch as pl\n",
        "\n",
        "# Cache the current directory\n",
        "exercise_dir = os.path.abspath(os.getcwd())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwn0XDvaSkH8"
      },
      "source": [
        "# **Documentation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_TQ6gHgSkIE"
      },
      "source": [
        "We're ready to implement modlee to document an image segmentation experiment.\n",
        "Please [review the documentation example](https://www.documentation.modlee.ai/notebooks/document.html) before continuing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4vnRZuooyh-"
      },
      "source": [
        "In the next cell, import `modlee` and initialize with an API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EewhsqCbSkIE"
      },
      "outputs": [],
      "source": [
        "# Your code goes here. Import the modlee package and initialize with your API key.\n",
        "# Import\n",
        "import\n",
        "# Initialize\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "898vf1RhSkIF"
      },
      "source": [
        "Download the training and validation datasets.\n",
        "This cell requires no modifications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAA-tr1ySkIF"
      },
      "outputs": [],
      "source": [
        "# Normalization parameters for ImageNet\n",
        "imagenet_mean = [0.485, 0.456, 0.406]  # mean of the imagenet dataset for normalizing\n",
        "imagenet_std = [0.229, 0.224, 0.225]  # std of the imagenet dataset for normalizing\n",
        "\n",
        "def replace_tensor_value_(tensor, a, b):\n",
        "    tensor[tensor == a] = b\n",
        "    return tensor\n",
        "\n",
        "input_resize = transforms.Resize((224, 224))\n",
        "input_transform = transforms.Compose(\n",
        "    [\n",
        "        input_resize,\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(imagenet_mean, imagenet_std),\n",
        "    ]\n",
        ")\n",
        "\n",
        "target_resize = transforms.Resize((224, 224), interpolation=InterpolationMode.NEAREST)\n",
        "target_transform = transforms.Compose(\n",
        "    [\n",
        "        target_resize,\n",
        "        transforms.PILToTensor(),\n",
        "        transforms.Lambda(lambda x: replace_tensor_value_(x.squeeze(0).long(), 255, 21)),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Creating the dataset\n",
        "train_dataset = torchvision.datasets.VOCSegmentation(\n",
        "    './datasets/',\n",
        "    year='2007',\n",
        "    download=True,\n",
        "    image_set='val',\n",
        "    transform=input_transform,\n",
        "    target_transform=target_transform,\n",
        ")\n",
        "val_dataset = torchvision.datasets.VOCSegmentation(\n",
        "    './datasets/',\n",
        "    year='2007',\n",
        "    download=True,\n",
        "    image_set='val',\n",
        "    transform=input_transform,\n",
        "    target_transform=target_transform,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Elc8tBm7irJJ"
      },
      "source": [
        "Load the datasets into data loaders.\n",
        "This cell requires no modification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gFsJAL3ijc-"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 16\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Cache an input to pass through the model later \n",
        "x, _ = next(iter(train_dataloader))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klsHMWCySkIG"
      },
      "source": [
        "In the next cell, we will construct the model.\n",
        "We initialize the model from a [pretrained fully connected network](https://pytorch.org/vision/main/models/generated/torchvision.models.segmentation.fcn_resnet50.html#torchvision.models.segmentation.fcn_resnet50).\n",
        "We subclass the `modlee.model.ModleeModel` parent class so that the experiment will automatically document.\n",
        "At minimum, you must define the `__init__()`, `forward()`, `training_step()`, and `configure_optimizers()` functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGwOhASRSkIG"
      },
      "outputs": [],
      "source": [
        "# Use a prerained torchvision Fully Connected Network\n",
        "fcn_model = torchvision.models.segmentation.fcn_resnet50(num_classes=22)\n",
        "\n",
        "# Subclass the correct modlee class\n",
        "class ModleeFCN( ''' Replace this with the correct modlee parent class '''):\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__()\n",
        "        self.model = # Set the above fcn object to self.model\n",
        "        self.loss_fn = F.cross_entropy\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Fill out the forward pass\n",
        "        # Should return a tensor after it has passed through the model\n",
        "        pass\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # Fill out the training step\n",
        "        x, y_target = # Get the input and output from the batch\n",
        "\n",
        "        # The segmentation dataset returns a dictionary,\n",
        "        # so extract the output tensor at 'out'\n",
        "        y_pred = self(x)['out']\n",
        "        loss = # Calculate the loss between the prediction and target\n",
        "        return {'loss': loss}\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # Fill out the optimizer configuration\n",
        "        pass\n",
        "\n",
        "# Create the model object\n",
        "modlee_model = ModleeFCN()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GHkRWeoSkIH"
      },
      "source": [
        "In the next cell, start training within a `modlee.start_run()` context manager."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yt-dAnpYSkIH"
      },
      "outputs": [],
      "source": [
        "# Your code goes here. Start training within a modlee.start_run() context manager\n",
        "# Create the context manager (with ... as ... :)\n",
        "with # Fill in the context manager (with ... as ...)\n",
        "    # Create the trainer object\n",
        "    trainer =\n",
        "    # Fit the trainer to the model and the training dataloader\n",
        "    trainer.fit(\n",
        "        # Fill in the arguments for training\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MevJ9kOdSkIH"
      },
      "source": [
        "Rebuild the saved model.\n",
        "First, determine the path to the most recent run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7TkXsQgSkIH"
      },
      "outputs": [],
      "source": [
        "last_run_path = # Get the last run path\n",
        "print(f\"Run path: {last_run_path}\")\n",
        "artifacts_path = os.path.join(last_run_path, 'artifacts')\n",
        "artifacts = os.listdir(artifacts_path)\n",
        "print(f\"Saved artifacts: {artifacts}\")\n",
        "os.environ['ARTIFACTS_PATH'] = artifacts_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csK3gO0uSkII"
      },
      "source": [
        "Next, reload the model from the assets saved in the `artifacts/` directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POvZhC2LSkII"
      },
      "outputs": [],
      "source": [
        "# Add the artifacts directory to the path, \n",
        "# so we can import the model\n",
        "sys.path.insert(0, artifacts_path)\n",
        "\n",
        "# Print out the first few lines of the model \n",
        "print(\"Model graph:\")\n",
        "!head $ARTIFACTS_PATH/model_graph.py\n",
        "print(\"\\n\")\n",
        "print(\"Data metafeatures:\")\n",
        "!head $ARTIFACTS_PATH/stats_rep\n",
        "\n",
        "# Import the model graph:\n",
        "import\n",
        "rebuilt_model = # Construct the model from the model graph module\n",
        "# Set the model to evaluation mode to turn off gradients\n",
        "rebuilt_model.eval()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we pass an input from the train dataloader (which we cached earlier) through the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pass an input through the model\n",
        "with torch.no_grad():\n",
        "    y_rebuilt = rebuilt_model(x)\n",
        "\n",
        "print(f\"Rebuilt output shape: {y_rebuilt.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76Z3ESkxoDuc"
      },
      "source": [
        "You have completed the documentation example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8dxupD3wxI4"
      },
      "source": [
        "# **Recommendation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzWbfSnppKUU"
      },
      "source": [
        "We're ready to implement a modlee-recommended model in an experiment. Please [review the recommendation example](https://www.documentation.modlee.ai/notebooks/recommend.html) before continuing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFvlvRvkxemu"
      },
      "source": [
        "We can skip the `modlee` initialization steps assuming we did so in the documentation example.\n",
        "First, we create a dataloader from CIFAR10.\n",
        "This cell does not need any modifications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebZDsx7xw0Rg"
      },
      "outputs": [],
      "source": [
        "transforms = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=transforms)\n",
        "val_dataset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=transforms)\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=16,\n",
        ")\n",
        "val_dataloader = torch.utils.data.DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=16\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DV_M16DxYJL"
      },
      "source": [
        "Create a `modlee` recommender object for an image classification task and fit to the dataset.\n",
        "This process will calculate the dataset metafeatures to send to the server.\n",
        "The server will return a recommended model for the dataset assigned to `recommender.model`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c69_xrPdxNk5"
      },
      "outputs": [],
      "source": [
        "recommender = # Create a recommender object for Image Classification\n",
        "recommender.fit(train_dataloader)\n",
        "modlee_model = # Extract the model from the recommender"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**TODO** Convert modlee model to graph text, print the first few lines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPXiDgHoxXXK"
      },
      "source": [
        "We can train the model as we would a basic `ModleeModel`, with automatic documentation of metafeatures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Va6b1o6qxQKW"
      },
      "outputs": [],
      "source": [
        "# Create and train the trainer\n",
        "with # ... as ...:\n",
        "    trainer = # Create the trainer\n",
        "    trainer.fit(\n",
        "        # Fill in the arguments\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxxYwqBbxSxq"
      },
      "source": [
        "Finally, we can view the saved assets from training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSBJqO0HxQ2x"
      },
      "outputs": [],
      "source": [
        "last_run_path = # Get the last run path\n",
        "print(f\"Run path: {last_run_path}\")\n",
        "artifacts_path = os.path.join(last_run_path, 'artifacts')\n",
        "artifacts = os.listdir(artifacts_path)\n",
        "print(f\"Saved artifacts: {artifacts}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7RTL8TWx0hV"
      },
      "source": [
        "# **Conclusion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SVL6IkVSkII"
      },
      "source": [
        "You've reached the end of the tutorial and can now implement `modlee` into your machine learning experiments.\n",
        "Congratulations!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
